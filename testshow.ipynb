{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "438625f1-af65-41ee-ba88-e5e40fa2726f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from datasets.dataset import load_data#, CLASS_MAP\n",
    "import models.models as models\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import argparse\n",
    "import csv\n",
    "\n",
    "import utils.my_trainer as trainer\n",
    "import utils.train_result as train_result\n",
    "from utils.data_class import BrainDataset\n",
    "import utils.confusion as confusion\n",
    "import torchio as tio\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d08143aa-8b70-4d83-adad-db4ab2e2dc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function line up input images and output images\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "def show_image(image, output):    \n",
    "    %matplotlib inline\n",
    "\n",
    "    fig = plt.figure(figsize=(18,5))\n",
    "    X, Y = 2, 8\n",
    "    \n",
    "    for i in range(8):\n",
    "        imgplot = i + 1\n",
    "        ax1 = fig.add_subplot(X, Y, imgplot)\n",
    "        ax1.set_title(\"original\"+str(imgplot), fontsize=12)\n",
    "        img = np.flip(image[i].numpy().reshape(80, 96, 80).transpose(1,2,0)[50],0)\n",
    "        plt.imshow(img, cmap=\"gray\")\n",
    "        plt.tick_params(labelsize=8)\n",
    "        \n",
    "        ax2 = fig.add_subplot(X, Y, imgplot+Y)\n",
    "        ax2.set_title(\"output\"+str(imgplot), fontsize=12) \n",
    "        out = np.flip(output[i].numpy().reshape(80, 96, 80).transpose(1,2,0)[50],0)\n",
    "        plt.imshow(out, cmap=\"gray\")\n",
    "        ax_pos = ax2.get_position()\n",
    "        mse_value = round(mean_squared_error(img, out), 3)\n",
    "        ssim_value = round(ssim(img, out), 3)\n",
    "        fig.text(ax_pos.x1 - 0.065, ax_pos.y1 - 0.4, \" mse: \" + str(mse_value), size=12)\n",
    "        fig.text(ax_pos.x1 - 0.065, ax_pos.y1 - 0.45, \"ssim: \" + str(ssim_value), size=12)\n",
    "        plt.tick_params(labelsize=8)\n",
    "    #fig.tight_layout()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d749b273-d7d2-4f76-bc40-fd74c7a822b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate ssim and mse\n",
    "\n",
    "def calc_ssim(image, output):\n",
    "    mse_sum = 0\n",
    "    ssim_sum = 0\n",
    "    for i in range(len(image)):\n",
    "        img = np.flip(image[i].numpy().reshape(80, 96, 80).transpose(1,2,0)[50],0)\n",
    "        out = np.flip(output[i].numpy().reshape(80, 96, 80).transpose(1,2,0)[50],0)\n",
    "\n",
    "        mse_value = mean_squared_error(img, out)\n",
    "        ssim_value = ssim(img, out)\n",
    "\n",
    "        mse_sum += mse_value\n",
    "        ssim_sum += ssim_value\n",
    "    return mse_sum / len(image), ssim_sum / len(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35e6f2fb-f2b0-40d4-9616-8c5e581abc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser():\n",
    "    parser = argparse.ArgumentParser(description=\"example\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=16)\n",
    "    parser.add_argument(\"--epoch\", type=int, default=100)\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.001)\n",
    "    parser.add_argument(\"--log\", type=str, default=\"output\")\n",
    "    parser.add_argument(\"--n_train\", type=float, default=0.8)\n",
    "    parser.add_argument(\"--train_or_loadnet\", type=str, default=\"loadnet\")    # train or loadnet\n",
    "    parser.add_argument(\"--model\", type=str, default=\"CAE\")     # VAE or CAE\n",
    "    args = parser.parse_args(args=['--model','SoftIntroVAE'])\n",
    "    return args\n",
    "\n",
    "\n",
    "# TorchIO\n",
    "class ImageTransformio():\n",
    "    def __init__(self):\n",
    "        self.transform = {\n",
    "            \"train\":tio.Compose([\n",
    "                #tio.RandomBlur(),\n",
    "                #tio.RandomBiasField(coefficients=1),\n",
    "                # tio.RandomAffine(scales=(0.9, 1.2), degrees=10, isotropic=True, center=\"image\", default_pad_value=\"mean\", image_interpolation='linear'),\n",
    "                #tio.ZNormalization(),\n",
    "                # tio.RescaleIntensity((0, 1))#, in_min_max=(0.1, 255))\n",
    "                \n",
    "            ]),\n",
    "            \"val\":tio.Compose([\n",
    "                #tio.ZNormalization(),\n",
    "                # tio.RescaleIntensity((0, 1))#, in_min_max=(0.1, 255))\n",
    "            ])\n",
    "        } \n",
    "    def __call__(self, img, phase=\"train\"):\n",
    "        # img_t = torch.unsqueeze(torch.tensor(img), 0) \n",
    "        img_t = torch.tensor(img)\n",
    "        return self.transform[phase](img_t)\n",
    "        \n",
    "def load_dataloader(n_train_rate, batch_size):\n",
    "#    data = load_data(kinds=[\"ADNI2\",\"ADNI2-2\"], classes=[\"CN\", \"AD\"], unique=False)\n",
    "    data = load_data(kinds=[\"ADNI2-2\"], classes=[\"CN\", \"AD\"], unique=True)\n",
    "\n",
    "    pids=[]\n",
    "    for i in range(len(data)):\n",
    "        pids.append(data[i][\"pid\"])\n",
    "    gss = GroupShuffleSplit(test_size=1-n_train_rate, random_state=0)\n",
    "    train_idx, val_idx = list(gss.split(data, groups=pids))[0]\n",
    "    train_data=data[train_idx]\n",
    "    val_data=data[val_idx]\n",
    "\n",
    "    #train_datadict, val_datadict = train_test_split(dataset, test_size=1-n_train_rate, shuffle=True)\n",
    "\n",
    "    train_dataset = BrainDataset(data_dict=train_data, transform=ImageTransformio(), phase=\"train\")\n",
    "    val_dataset = BrainDataset(data_dict=val_data, transform=ImageTransformio(), phase=\"val\")\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=os.cpu_count(), pin_memory=True, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, num_workers=os.cpu_count(), pin_memory=True, shuffle=False)\n",
    "\n",
    "    return train_dataloader, val_dataloader\n",
    "\n",
    "#os.makedirs(log_path, exist_ok=True)\n",
    "#os.makedirs(log_path + \"csv/\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5738cb5b-0ff3-435a-8401-ba8d7a4564f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser()# args使うときはこれが必要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f4a83ea-c81c-4b82-86fe-870d9f5cc7c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SoftIntroVAE'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f743772-e20b-45d9-900d-6ee2d86e6f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.n_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bc6c326-6b27-498a-8f53-8d5cf1293ad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49c13526-6e73-47e6-b194-fefee649071f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                    \r"
     ]
    }
   ],
   "source": [
    "data = load_data(kinds=[\"ADNI2\", \"ADNI2-2\"], classes=[\"CN\", \"AD\"], unique=False)\n",
    "n_train_rate = args.n_train\n",
    "batch_size = 16\n",
    "pids=[]\n",
    "for i in range(len(data)):\n",
    "    pids.append(data[i][\"pid\"])\n",
    "gss = GroupShuffleSplit(test_size=1-n_train_rate, random_state=0)\n",
    "train_idx, val_idx = list(gss.split(data, groups=pids))[0]\n",
    "train_data=data[train_idx]\n",
    "val_data=data[val_idx]\n",
    "\n",
    "#train_datadict, val_datadict = train_test_split(dataset, test_size=1-n_train_rate, shuffle=True)\n",
    "\n",
    "train_dataset = BrainDataset(data_dict=train_data, transform=ImageTransformio(), phase=\"train\")\n",
    "val_dataset = BrainDataset(data_dict=val_data, transform=ImageTransformio(), phase=\"val\")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=os.cpu_count(), pin_memory=True, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, num_workers=os.cpu_count(), pin_memory=True, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b22d311-d79a-4f96-a279-2b9fc5acd981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7ff8dfa4b6a0>\n"
     ]
    }
   ],
   "source": [
    "print(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e29ef546-03d6-4609-8695-b338716ec57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, label in train_dataset:\n",
    "    images, label = images, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2d326e2-6d61-41a1-9031-fb8b2c64b6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 80, 96, 80])\n"
     ]
    }
   ],
   "source": [
    "print(images.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8430bfd-6b33-4c88-bfb7-cfe03ef1870d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1939\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "914c948a-9027-42a4-a52c-bda83dac380f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe644189-b57e-4877-9897-f8ed8caef653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CN'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c345ff71-257e-4f97-8692-1e9698a4f7a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1198"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(data)):\n",
    "    if data[i]['label'] == 'CN':\n",
    "        count += 1\n",
    "        \n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15190b9b-c04c-4db3-ad23-bd702f0fdeb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "args = parser()\n",
    "args.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c7e5ad8-7b5d-4fbe-8e22-3314b2a1754d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BuildingBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, stride, bias=False):\n",
    "        super(BuildingBlock, self).__init__()\n",
    "        self.res = stride == 1\n",
    "        self.shortcut = self._shortcut()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv3d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=bias),\n",
    "            nn.BatchNorm3d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AvgPool3d(kernel_size=stride),\n",
    "            nn.Conv3d(out_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=bias),\n",
    "            nn.BatchNorm3d(out_ch),\n",
    "        )\n",
    "\n",
    "    def _shortcut(self):\n",
    "        return lambda x: x\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.res:\n",
    "            shortcut = self.shortcut(x)\n",
    "            return self.relu(self.block(x) + shortcut)\n",
    "        else:\n",
    "            return self.relu(self.block(x))\n",
    "\n",
    "class UpsampleBuildingkBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, stride, bias=False):\n",
    "        super(UpsampleBuildingkBlock, self).__init__()\n",
    "        self.res = stride == 1\n",
    "        self.shortcut = self._shortcut()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv3d(in_ch, in_ch, kernel_size=3, stride=1, padding=1, bias=bias),\n",
    "            nn.BatchNorm3d(in_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=stride),\n",
    "            nn.Conv3d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=bias),\n",
    "            nn.BatchNorm3d(out_ch),\n",
    "        )\n",
    "\n",
    "    def _shortcut(self):\n",
    "        return lambda x: x\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.res:\n",
    "            shortcut = self.shortcut(x)\n",
    "            return self.relu(self.block(x) + shortcut)\n",
    "        else:\n",
    "            return self.relu(self.block(x))\n",
    "\n",
    "\n",
    "class ResNetEncoder(nn.Module):\n",
    "    def __init__(self, in_ch, block_setting):\n",
    "        super(ResNetEncoder, self).__init__()\n",
    "        self.block_setting = block_setting\n",
    "        self.in_ch = in_ch\n",
    "        last = 1\n",
    "        blocks = [nn.Sequential(\n",
    "            nn.Conv3d(1, in_ch, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm3d(in_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )]\n",
    "        for line in self.block_setting:\n",
    "            c, n, s = line[0], line[1], line[2]\n",
    "            for i in range(n):\n",
    "                stride = s if i == 0 else 1\n",
    "                blocks.append(nn.Sequential(BuildingBlock(in_ch, c, stride)))\n",
    "                in_ch = c\n",
    "        self.inner_ch = in_ch\n",
    "        self.blocks = nn.Sequential(*blocks)\n",
    "        self.conv = nn.Conv3d(in_ch, last, kernel_size=1, stride=1, bias=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        h = self.blocks(x)\n",
    "        return self.conv(h)\n",
    "\n",
    "class ResNetDecoder(nn.Module):\n",
    "    def __init__(self, encoder: ResNetEncoder, blocks=None):\n",
    "        super(ResNetDecoder, self).__init__()\n",
    "        last = encoder.block_setting[-1][0]\n",
    "        if blocks is None:\n",
    "            blocks = [nn.Sequential(\n",
    "                nn.Conv3d(1, last, 1, 1, bias=True),\n",
    "                nn.BatchNorm3d(last),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )]\n",
    "        in_ch = last\n",
    "        for i in range(len(encoder.block_setting)):\n",
    "            if i == len(encoder.block_setting) - 1:\n",
    "                nc = encoder.in_ch\n",
    "            else:\n",
    "                nc = encoder.block_setting[::-1][i + 1][0]\n",
    "            c, n, s = encoder.block_setting[::-1][i]\n",
    "            for j in range(n):\n",
    "                stride = s if j == n - 1 else 1\n",
    "                c = nc if j == n - 1 else c\n",
    "                blocks.append(nn.Sequential(UpsampleBuildingkBlock(in_ch, c, stride)))\n",
    "                in_ch = c\n",
    "        blocks.append(nn.Sequential(\n",
    "            nn.Conv3d(in_ch, 1, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.ReLU(),\n",
    "        ))\n",
    "        self.blocks = nn.Sequential(*blocks)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.blocks(x)\n",
    "\n",
    "\n",
    "class BaseEncoder(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(BaseEncoder, self).__init__()\n",
    "class BaseDecoder(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(BaseDecoder, self).__init__()\n",
    "\n",
    "class BaseCAE(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(BaseCAE, self).__init__()\n",
    "        self.encoder = BaseEncoder()\n",
    "        self.decoder = BaseDecoder()\n",
    "    def encode(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return z\n",
    "    def decode(self, z):\n",
    "        out = self.decoder(z)\n",
    "        return out\n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        out = self.decode(z)\n",
    "        return out, z\n",
    "\n",
    "class BaseVAE(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(BaseVAE, self).__init__()\n",
    "        self.encoder = BaseEncoder()\n",
    "        self.decoder = BaseDecoder()\n",
    "    def encode(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        return mu, logvar\n",
    "    def decode(self, vec):\n",
    "        out = self.decoder(vec)\n",
    "        return out\n",
    "    def reparameterize(self, mu, logvar) -> torch.Tensor:\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        vec = self.reparameterize(mu, logvar)\n",
    "        x_hat = self.decode(vec)\n",
    "        return x_hat, vec, mu, logvar\n",
    "\n",
    "\n",
    "class ResNetCAE(BaseCAE):\n",
    "    def __init__(self, in_ch, block_setting) -> None:\n",
    "        super(ResNetCAE, self).__init__()\n",
    "        self.encoder = ResNetEncoder(\n",
    "            in_ch=in_ch,\n",
    "            block_setting=block_setting,\n",
    "        )\n",
    "        self.decoder = ResNetDecoder(self.encoder)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.forward(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class VAEResNetEncoder(ResNetEncoder):\n",
    "    def __init__(self, in_ch, block_setting) -> None:\n",
    "        super(VAEResNetEncoder, self).__init__(in_ch, block_setting)\n",
    "        self.mu = nn.Conv3d(self.inner_ch, 1, kernel_size=1, stride=1, bias=True)\n",
    "        self.var = nn.Conv3d(self.inner_ch, 1, kernel_size=1, stride=1, bias=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        h = self.blocks(x)\n",
    "        mu = self.mu(h)\n",
    "        var = self.var(h)\n",
    "        return mu, var\n",
    "\n",
    "\n",
    "class ResNetVAE(BaseVAE):\n",
    "    def __init__(self, in_ch, block_setting) -> None:\n",
    "        super(ResNetVAE, self).__init__()\n",
    "        self.encoder = VAEResNetEncoder(\n",
    "            in_ch=in_ch,\n",
    "            block_setting=block_setting,\n",
    "        )\n",
    "        self.decoder = ResNetDecoder(self.encoder)\n",
    "\n",
    "    def reparamenterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparamenterize(mu, logvar)\n",
    "        x_re = self.decoder(z)\n",
    "        return x_re, mu, logvar\n",
    "\n",
    "    def loss(self, x_re, x, mu, logvar):\n",
    "        re_err = torch.sqrt(torch.mean((x_re - x)**2)) # ==  self.Rmse(x_re, x)\n",
    "        kld = -0.5 * torch.sum(1 + logvar - mu**2 - logvar.exp())\n",
    "        return re_err + kld\n",
    "\n",
    "\n",
    "\n",
    "class SoftIntroVAE(nn.Module):\n",
    "    def __init__(self, in_ch, block_setting, zdim=150, conditional=False):\n",
    "        super(SoftIntroVAE, self).__init__()\n",
    "        self.zdim = zdim\n",
    "        self.conditional = conditional\n",
    "        self.encoder = VAEResNetEncoder(\n",
    "            in_ch=in_ch,\n",
    "            block_setting=block_setting,\n",
    "        )\n",
    "        self.decoder = ResNetDecoder(self.encoder)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_re = self.decoder(z)\n",
    "        return mu, logvar, z, x_re\n",
    "#     ↑ここの forward では  RETURN {{ mu, logvar, z, y }}を返したい (soft-intro-vae-tutorial-codeでは)\n",
    "\n",
    "    def loss(self, x_re, x, mu, logvar):\n",
    "        re_err = torch.sqrt(torch.mean((x_re - x)**2)) # ==  self.Rmse(x_re, x)\n",
    "        kld = -0.5 * torch.sum(1 + logvar - mu**2 - logvar.exp())\n",
    "        return re_err + kld\n",
    "\n",
    "    def sample(self, z, y_cond=None):\n",
    "        z = z.view(32, 1, 5, 6, 5)# batchsize, channel, 5×6×5 (150)\n",
    "        y = self.decode(z, y_cond=y_cond)\n",
    "        return y\n",
    "\n",
    "    def sample_with_noise(self, num_samples=1, device=torch.device(\"cpu\"), y_cond=None):\n",
    "        z = torch.randn(num_samples, self.z_dim).to(device)\n",
    "        return self.decode(z, y_cond=y_cond)\n",
    "\n",
    "    def encode(self, x, o_cond=None):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        return mu, logvar\n",
    "\n",
    "    def decode(self, z, y_cond=None):\n",
    "        y = self.decoder(z)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3db19984-2e82-448b-8336-a2fcef3732c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SoftIntroVAE\n",
      "tensor([[[-0.0435, -0.0435, -0.0435, -0.0435, -0.0435],\n",
      "         [-0.0435, -0.0435, -0.0435, -0.0435, -0.0435],\n",
      "         [-0.0435, -0.0435, -0.0435, -0.0435, -0.0435],\n",
      "         [-0.0435, -0.0435, -0.0435, -0.0435, -0.0435],\n",
      "         [-0.0435, -0.0435, -0.0435, -0.0435, -0.0435],\n",
      "         [-0.0435, -0.0435, -0.0435, -0.0435, -0.0435]],\n",
      "\n",
      "        [[-0.0435, -0.0434, -0.0434, -0.0434, -0.0435],\n",
      "         [-0.0435, -0.0435, -0.0435, -0.0435, -0.0435],\n",
      "         [-0.0435, -0.0435, -0.0435, -0.0435, -0.0435],\n",
      "         [-0.0435, -0.0435, -0.0435, -0.0435, -0.0435],\n",
      "         [-0.0435, -0.0435, -0.0435, -0.0435, -0.0435],\n",
      "         [-0.0435, -0.0435, -0.0435, -0.0436, -0.0435]],\n",
      "\n",
      "        [[-0.0435, -0.0435, -0.0435, -0.0434, -0.0435],\n",
      "         [-0.0435, -0.0435, -0.0435, -0.0435, -0.0435],\n",
      "         [-0.0435, -0.0435, -0.0435, -0.0435, -0.0435],\n",
      "         [-0.0435, -0.0435, -0.0435, -0.0435, -0.0435],\n",
      "         [-0.0435, -0.0435, -0.0435, -0.0435, -0.0435],\n",
      "         [-0.0435, -0.0435, -0.0435, -0.0436, -0.0435]],\n",
      "\n",
      "        [[-0.0435, -0.0434, -0.0434, -0.0434, -0.0435],\n",
      "         [-0.0435, -0.0435, -0.0435, -0.0435, -0.0435],\n",
      "         [-0.0435, -0.0435, -0.0435, -0.0435, -0.0435],\n",
      "         [-0.0435, -0.0435, -0.0435, -0.0435, -0.0435],\n",
      "         [-0.0435, -0.0435, -0.0435, -0.0435, -0.0435],\n",
      "         [-0.0435, -0.0435, -0.0435, -0.0435, -0.0435]],\n",
      "\n",
      "        [[-0.0435, -0.0434, -0.0434, -0.0434, -0.0435],\n",
      "         [-0.0435, -0.0435, -0.0435, -0.0435, -0.0435],\n",
      "         [-0.0435, -0.0435, -0.0435, -0.0435, -0.0435],\n",
      "         [-0.0435, -0.0435, -0.0435, -0.0435, -0.0435],\n",
      "         [-0.0435, -0.0435, -0.0435, -0.0435, -0.0435],\n",
      "         [-0.0435, -0.0435, -0.0435, -0.0435, -0.0435]]], device='cuda:1')\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'cpu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28mprint\u001b[39m(output[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     36\u001b[0m     exit\n\u001b[0;32m---> 37\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m()\n\u001b[1;32m     39\u001b[0m     show_image(image, output)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVAE\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'cpu'"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() and True else \"cpu\")\n",
    "print(\"device:\", device)    \n",
    "\n",
    "train_loader, val_loader = load_dataloader(args.n_train, args.batch_size)\n",
    "\n",
    "if args.model == \"SoftIntroVAE\":\n",
    "    print(\"SoftIntroVAE\")\n",
    "    log_path = \"./logs/\" + args.log + \"_SoftIntroVAE/\"\n",
    "    os.makedirs(log_path, exist_ok=True)\n",
    "\n",
    "    net = SoftIntroVAE(12, [[12,1,2],[24,1,2],[32,2,2],[48,2,2]])\n",
    "    weight_name = 'softintrovae_weight_epoch298.pth'\n",
    "#    net.load_state_dict( torch.load(log_path + weight_name) )\n",
    "#    net.load_state_dict(torch.load(log_path+'softintrovae_weight_epoch298.pth'), strict=False)\n",
    "# original saved file with DataParallel\n",
    "    state_dict = torch.load(log_path + weight_name)\n",
    "# create new OrderedDict that does not contain `module.`\n",
    "#     from collections import OrderedDict\n",
    "#     new_state_dict = OrderedDict()\n",
    "#     for k, v in state_dict.items():\n",
    "#         name = k[0:] # remove `module.`\n",
    "#         new_state_dict[name] = v\n",
    "# # load params\n",
    "#     net.load_state_dict(new_state_dict)\n",
    "\n",
    "    val_loader_iter = iter(train_loader)\n",
    "    image, _ = next(val_loader_iter)\n",
    "    image = image.to(device)\n",
    "    net.to(device)\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        output = net(image)\n",
    "\n",
    "    image = image.cpu()\n",
    "    print(output[0][0][0])\n",
    "    exit\n",
    "    output = output.cpu()\n",
    "\n",
    "    show_image(image, output)\n",
    "\n",
    "\n",
    "elif args.model == \"VAE\":\n",
    "    print(\"VAE\")\n",
    "    log_path = \"./logs/\" + args.log + \"_vae/\"\n",
    "\n",
    "    os.makedirs(log_path, exist_ok=True)\n",
    "    net = models.Vae()\n",
    "    net.load_state_dict(torch.load(log_path+'vae_weight.pth'))\n",
    "    val_loader_iter = iter(val_loader)\n",
    "    image, _ = next(val_loader_iter)\n",
    "    image = image.to(device)\n",
    "    net.to(device)\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        output, _, _ = net.forward(image)\n",
    "\n",
    "    image = image.cpu()\n",
    "    output = output.cpu()\n",
    "    show_image(image, output)\n",
    "    mse, ssim = calc_ssim(image, output)\n",
    "    print(mse, ssim)\n",
    "    # pil_img = Image.fromarray(np.flip(output[0].numpy().reshape(80, 80, 80).transpose(1,2,0)[50],0) * 255)\n",
    "    # pil_img = pil_img.convert(\"L\")\n",
    "    #pil_img.save(log_path+\"img/vae_output_img.jpg\")\n",
    "\n",
    "\n",
    "elif args.model == \"CAE\":\n",
    "    print(\"CAE\")\n",
    "    log_path = \"./logs/\" + args.log + \"_cae/\"\n",
    "    os.makedirs(log_path, exist_ok=True)\n",
    "    net = models.Cae()\n",
    "    net.load_state_dict(torch.load(log_path+'cae_weight.pth'))\n",
    "    val_loader_iter = iter(train_loader)\n",
    "    image, _ = next(val_loader_iter)\n",
    "    image = image.to(device)\n",
    "    net.to(device)\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        output = net(image)\n",
    "\n",
    "    image = image.cpu()\n",
    "    output = output.cpu()\n",
    "    show_image(image, output)\n",
    "    \n",
    "    \n",
    "\n",
    "elif args.model == \"Cae_nearest\":\n",
    "    print(\"Cae_nearest\")\n",
    "    log_path = \"./logs/\" + args.log + \"_cae_nearest/\"\n",
    "    os.makedirs(log_path, exist_ok=True)\n",
    "    net = models.Cae()\n",
    "    net.load_state_dict(torch.load(log_path+'cae_nearest_weight_200.pth'))\n",
    "    val_loader_iter = iter(train_loader)\n",
    "    image, _ = next(val_loader_iter)\n",
    "    image = image.to(device)\n",
    "    net.to(device)\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        output = net(image)\n",
    "\n",
    "    image = image.cpu()\n",
    "    output = output.cpu()\n",
    "    show_image(image, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e2e59a-3fb2-4f42-97f7-a3553e644816",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_output_list = []\n",
    "label_list = []\n",
    "\n",
    "net.to(device)\n",
    "net.eval()\n",
    "train_loader_iter = iter(train_loader)\n",
    "\n",
    "for image, label in train_loader_iter:\n",
    "    image = image.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encoder_output = net.encoder(image)\n",
    "    encoder_output = encoder_output.cpu()\n",
    "    label = label.cpu()\n",
    "    encoder_output_list.append(encoder_output)\n",
    "    label_list.append(label)\n",
    "\n",
    "print(len(encoder_output_list))\n",
    "print(len(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "136eb43c-c6eb-46c4-a947-e31a6f18608c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encoder_output_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m tsne \u001b[38;5;241m=\u001b[39m TSNE(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, random_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, perplexity \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m, n_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_loader)):\n\u001b[0;32m---> 11\u001b[0m     X_embedded \u001b[38;5;241m=\u001b[39m tsne\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mencoder_output_list\u001b[49m[idx])\n\u001b[1;32m     12\u001b[0m     X_embedded_list\u001b[38;5;241m.\u001b[39mappend(X_embedded)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(idx)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'encoder_output_list' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "X_embedded_list = []\n",
    "color_list = []\n",
    "\n",
    "#t-SNEで次元削減\n",
    "tsne = TSNE(n_components=2, random_state = 0, perplexity = 30, n_iter = 1000)\n",
    "for idx in range(len(train_loader)):\n",
    "    X_embedded = tsne.fit_transform(encoder_output_list[idx])\n",
    "    X_embedded_list.append(X_embedded)\n",
    "    print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd799b4f-af99-45d1-a485-3b66e588ad8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embeddeds = np.concatenate(X_embedded_list)\n",
    "labels = np.concatenate(label_list)\n",
    "#print(X_embeddeds[labels==0])\n",
    "#print(labels)\n",
    "plt.figure(figsize = (10, 10))\n",
    "\n",
    "plt.scatter(X_embeddeds[labels==0, 0], X_embeddeds[labels==0, 1], c=\"r\", label=\"CN\")\n",
    "plt.scatter(X_embeddeds[labels==1, 0], X_embeddeds[labels==1, 1], c=\"g\", label=\"AD\")\n",
    "plt.grid()\n",
    "plt.legend(fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add59ed8-a78a-480d-b336-3702426393ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a776a7d8-3774-49fa-ba08-9eacfc91ce68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d2d5a5-3b79-43dc-a924-f4fb84aeca3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c5e9a0-f501-4e39-aeab-b989f4457823",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "83703930f6dd6eb5d91c8a9abe31f591cc3731721361839549be9d5bb51ffcf3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
